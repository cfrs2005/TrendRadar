"""
AI å¢å¼ºæœåŠ¡

ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½å»é‡ã€å†…å®¹èšç±»å’Œæ€»ç»“
"""

import os
import json
import hashlib
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import defaultdict
import requests


class AIEnhancedService:
    """AI å¢å¼ºæœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– AI å¢å¼ºæœåŠ¡"""
        self.token = os.environ.get('CLAUDE_CODE_OAUTH_TOKEN')
        self.api_url = "https://api.anthropic.com/v1/messages"
        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
    def _generate_content_hash(self, title: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡"""
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤æ—¶é—´æˆ³ã€æ’åç­‰å˜åŒ–éƒ¨åˆ†
        cleaned_title = self._clean_title_for_hash(title)
        return hashlib.md5(cleaned_title.encode()).hexdigest()[:8]
    
    def _clean_title_for_hash(self, title: str) -> str:
        """æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤å˜åŒ–çš„éƒ¨åˆ†"""
        import re
        # ç§»é™¤æ•°å­—å’Œæ—¶é—´ä¿¡æ¯
        cleaned = re.sub(r'\d+', '', title)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned)
        # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned = ' '.join(cleaned.lower().split())
        return cleaned
    
    def _call_claude_api(self, prompt: str, max_tokens: int = 4000) -> Optional[str]:
        """è°ƒç”¨ Claude API"""
        if not self.token:
            print("è­¦å‘Šï¼šæœªè®¾ç½® CLAUDE_CODE_OAUTH_TOKEN ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡ AI å¢å¼º")
            return None
            
        try:
            payload = {
                "model": "claude-3-haiku-20240307",
                "max_tokens": max_tokens,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("content", [{}])[0].get("text", "")
            else:
                print(f"API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"è°ƒç”¨ Claude API æ—¶å‡ºé”™: {e}")
            return None
    
    def deduplicate_news(self, news_items: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        æ™ºèƒ½å»é‡æ–°é—»
        
        Args:
            news_items: æ–°é—»åˆ—è¡¨
            
        Returns:
            (å»é‡åçš„æ–°é—»åˆ—è¡¨, é‡å¤ä¿¡æ¯ç»Ÿè®¡)
        """
        if not news_items:
            return [], {}
            
        # é¦–å…ˆè¿›è¡ŒåŸºäºå“ˆå¸Œçš„å¿«é€Ÿå»é‡
        seen_hashes = {}
        unique_items = []
        duplicate_groups = defaultdict(list)
        
        for item in news_items:
            content_hash = self._generate_content_hash(item['title'])
            
            if content_hash in seen_hashes:
                # æ·»åŠ åˆ°é‡å¤ç»„
                existing_item = seen_hashes[content_hash]
                duplicate_key = f"{content_hash}_{existing_item['title'][:20]}"
                duplicate_groups[duplicate_key].append({
                    'original': existing_item,
                    'duplicate': item,
                    'similarity': self._calculate_text_similarity(
                        existing_item['title'], item['title']
                    )
                })
            else:
                seen_hashes[content_hash] = item
                unique_items.append(item)
        
        # ä½¿ç”¨ AI è¿›ä¸€æ­¥ä¼˜åŒ–å»é‡ç»“æœ
        if len(unique_items) > 1:
            unique_items = self._ai_deduplicate(unique_items)
            
        duplicate_stats = {
            'original_count': len(news_items),
            'unique_count': len(unique_items),
            'removed_count': len(news_items) - len(unique_items),
            'duplicate_groups': dict(duplicate_groups)
        }
        
        return unique_items, duplicate_stats
    
    def _ai_deduplicate(self, news_items: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ AI è¿›è¡Œæ›´ç²¾ç¡®çš„å»é‡"""
        # å¦‚æœæ–°é—»æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥è¿”å›
        if len(news_items) <= 5:
            return news_items
            
        # æ„å»ºæ–°é—»æ ‡é¢˜åˆ—è¡¨
        titles = [item['title'] for item in news_items]
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼Œè¯†åˆ«å¹¶åˆå¹¶ç›¸ä¼¼æˆ–é‡å¤çš„å†…å®¹ã€‚è¿”å›å»é‡åçš„æ ‡é¢˜ç´¢å¼•åˆ—è¡¨ã€‚

æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š
{json.dumps(titles, ensure_ascii=False, indent=2)}

è¯·è¿”å›ä¸€ä¸ªJSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«ï¼š
1. "unique_indices": ä¿ç•™çš„æ–°é—»æ ‡é¢˜ç´¢å¼•åˆ—è¡¨ï¼ˆä»0å¼€å§‹ï¼‰
2. "reason": ç®€è¦è¯´æ˜å»é‡çš„ç†ç”±

ç¤ºä¾‹è¿”å›æ ¼å¼ï¼š
{{
    "unique_indices": [0, 2, 4, 7],
    "reason": "å»é™¤äº†å†…å®¹ç›¸ä¼¼çš„é‡å¤æŠ¥é“"
}}

æ³¨æ„ï¼š
- ä¿ç•™æœ€é‡è¦ã€ä¿¡æ¯æœ€å®Œæ•´çš„æ ‡é¢˜
- åŒä¸€äº‹ä»¶çš„ä¸åŒè§’åº¦æŠ¥é“å¯ä»¥é€‚å½“ä¿ç•™
- ä¼˜å…ˆä¿ç•™æ’åè¾ƒé«˜çš„æ–°é—»
"""

        response = self._call_claude_api(prompt)
        if not response:
            return news_items
            
        try:
            # è§£æ AI å“åº”
            result = json.loads(response)
            if 'unique_indices' in result:
                unique_indices = result['unique_indices']
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                unique_indices = [i for i in unique_indices if 0 <= i < len(news_items)]
                return [news_items[i] for i in unique_indices]
        except (json.JSONDecodeError, KeyError) as e:
            print(f"è§£æ AI å»é‡ç»“æœæ—¶å‡ºé”™: {e}")
            
        return news_items
    
    def categorize_and_summarize(self, news_items: List[Dict]) -> Dict:
        """
        å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»å’Œæ€»ç»“
        
        Args:
            news_items: æ–°é—»åˆ—è¡¨
            
        Returns:
            åˆ†ç±»å’Œæ€»ç»“ç»“æœ
        """
        if not news_items:
            return {'categories': {}, 'summary': '', 'total_count': 0}
            
        # æŒ‰å¹³å°åˆ†ç»„
        platform_groups = defaultdict(list)
        for item in news_items:
            platform_groups[item['platform_name']].append(item)
        
        # æ„å»ºåˆ†ç±»æç¤º
        news_content = []
        for i, item in enumerate(news_items):
            news_content.append(f"{i+1}. [{item['platform_name']}] {item['title']}")
        
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œæ™ºèƒ½åˆ†ç±»å’Œæ€»ç»“åˆ†æï¼š

æ–°é—»å†…å®¹ï¼š
{chr(10).join(news_content)}

è¯·æä¾›ä»¥ä¸‹åˆ†æç»“æœï¼Œè¿”å›JSONæ ¼å¼ï¼š

{{
    "categories": [
        {{
            "name": "åˆ†ç±»åç§°",
            "count": æ•°é‡,
            "items": [ç´¢å¼•1, ç´¢å¼•2, ...],
            "description": "ç®€çŸ­æè¿°"
        }}
    ],
    "summary": "æ•´ä½“æ€»ç»“ï¼Œçªå‡ºé‡è¦ä¿¡æ¯å’Œè¶‹åŠ¿",
    "trending_topics": ["çƒ­é—¨è¯é¢˜1", "çƒ­é—¨è¯é¢˜2", ...],
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2", ...]
}}

åˆ†ç±»è¦æ±‚ï¼š
- æŒ‰å†…å®¹ä¸»é¢˜åˆ†ç±»ï¼ˆå¦‚ç§‘æŠ€ã€è´¢ç»ã€ç¤¾ä¼šç­‰ï¼‰
- æ¯ä¸ªåˆ†ç±»åŒ…å«2-8æ¡æ–°é—»
- çªå‡ºé‡è¦å’Œçƒ­é—¨çš„è¯é¢˜
- æ€»ç»“è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºå…³é”®ä¿¡æ¯
"""

        response = self._call_claude_api(prompt, max_tokens=6000)
        if not response:
            # è¿”å›åŸºç¡€åˆ†ç±»ç»“æœ
            return self._basic_categorization(news_items, platform_groups)
            
        try:
            result = json.loads(response)
            
            # æ·»åŠ å¹³å°ç»Ÿè®¡ä¿¡æ¯
            result['platform_stats'] = {
                name: len(items) for name, items in platform_groups.items()
            }
            result['total_count'] = len(news_items)
            
            return result
        except (json.JSONDecodeError, KeyError) as e:
            print(f"è§£æ AI åˆ†ç±»ç»“æœæ—¶å‡ºé”™: {e}")
            return self._basic_categorization(news_items, platform_groups)
    
    def _basic_categorization(self, news_items: List[Dict], platform_groups: Dict) -> Dict:
        """åŸºç¡€åˆ†ç±»åŠŸèƒ½ï¼ˆAI ä¸å¯ç”¨æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        # æŒ‰å¹³å°ç®€å•åˆ†ç±»
        categories = []
        for platform_name, items in platform_groups.items():
            categories.append({
                'name': f'{platform_name} å¹³å°',
                'count': len(items),
                'items': [news_items.index(item) for item in items],
                'description': f'æ¥è‡ª{platform_name}çš„æ–°é—»'
            })
        
        return {
            'categories': categories,
            'summary': f'å…±æ”¶é›†åˆ° {len(news_items)} æ¡æ–°é—»ï¼Œæ¥è‡ª {len(platform_groups)} ä¸ªå¹³å°ã€‚',
            'trending_topics': [],
            'key_insights': [],
            'platform_stats': {name: len(items) for name, items in platform_groups.items()},
            'total_count': len(news_items)
        }
    
    def generate_enhanced_message(self, news_items: List[Dict], categorization_result: Dict, duplicate_stats: Dict) -> str:
        """
        ç”Ÿæˆå¢å¼ºçš„æ¨é€æ¶ˆæ¯
        
        Args:
            news_items: å»é‡åçš„æ–°é—»åˆ—è¡¨
            categorization_result: åˆ†ç±»ç»“æœ
            duplicate_stats: å»é‡ç»Ÿè®¡
            
        Returns:
            æ ¼å¼åŒ–çš„æ¨é€æ¶ˆæ¯
        """
        if not categorization_result:
            return self._generate_basic_message(news_items)
            
        message_parts = []
        
        # æ·»åŠ å¤´éƒ¨ä¿¡æ¯
        total_original = duplicate_stats.get('original_count', len(news_items))
        total_unique = duplicate_stats.get('unique_count', len(news_items))
        removed_count = duplicate_stats.get('removed_count', 0)
        
        if removed_count > 0:
            message_parts.append(f"ğŸ¤– **AI æ™ºèƒ½å»é‡**: {total_original} æ¡ â†’ {total_unique} æ¡ (å»é™¤ {removed_count} æ¡é‡å¤)")
        
        # æ·»åŠ æ€»ç»“
        summary = categorization_result.get('summary', '')
        if summary:
            message_parts.append(f"ğŸ“ **å†…å®¹æ€»ç»“**: {summary}")
        
        # æ·»åŠ çƒ­é—¨è¯é¢˜
        trending_topics = categorization_result.get('trending_topics', [])
        if trending_topics:
            topics_str = 'ã€'.join(trending_topics[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ª
            message_parts.append(f"ğŸ”¥ **çƒ­é—¨è¯é¢˜**: {topics_str}")
        
        message_parts.append("â”" * 25)
        
        # æ·»åŠ åˆ†ç±»æ–°é—»
        categories = categorization_result.get('categories', [])
        for i, category in enumerate(categories, 1):
            if i > 5:  # é™åˆ¶æ˜¾ç¤º5ä¸ªåˆ†ç±»
                break
                
            category_name = category['name']
            category_items = category['items'][:3]  # æ¯ä¸ªåˆ†ç±»æœ€å¤šæ˜¾ç¤º3æ¡
            
            message_parts.append(f"**[{i}] {category_name}** ({category['count']} æ¡)")
            
            for item_idx in category_items:
                if item_idx < len(news_items):
                    item = news_items[item_idx]
                    rank_display = self._format_rank(item.get('rank', 99))
                    platform_display = item.get('platform_name', item.get('platform', ''))
                    
                    # ä¿ç•™ URL ä¿¡æ¯
                    title = item['title']
                    url = item.get('url', '')
                    mobile_url = item.get('mobileUrl', '')
                    
                    # æ ¼å¼åŒ–æ˜¾ç¤º
                    if url:
                        # å¦‚æœæœ‰ URLï¼Œæ·»åŠ é“¾æ¥
                        title_with_link = f"[{title}]({url})"
                    else:
                        title_with_link = title
                    
                    message_parts.append(
                        f"  {rank_display} {platform_display}: {title_with_link}"
                    )
            
            message_parts.append("")  # åˆ†ç±»é—´ç©ºè¡Œ
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        platform_stats = categorization_result.get('platform_stats', {})
        if platform_stats:
            stats_str = ', '.join([f"{name}({count})" for name, count in platform_stats.items()])
            message_parts.append(f"ğŸ“Š **å¹³å°åˆ†å¸ƒ**: {stats_str}")
        
        return '\n'.join(message_parts)
    
    def _generate_basic_message(self, news_items: List[Dict]) -> str:
        """ç”ŸæˆåŸºç¡€æ¶ˆæ¯ï¼ˆAI ä¸å¯ç”¨æ—¶ï¼‰"""
        if not news_items:
            return "æš‚æ— æ–°é—»"
            
        message_parts = ["ğŸ“Š **çƒ­ç‚¹æ–°é—»æ±‡æ€»**"]
        
        # æŒ‰å¹³å°åˆ†ç»„æ˜¾ç¤º
        platform_groups = defaultdict(list)
        for item in news_items:
            platform_groups[item.get('platform_name', item.get('platform', ''))].append(item)
        
        for platform_name, items in platform_groups.items():
            message_parts.append(f"\n**{platform_name}**")
            for item in items[:5]:  # æ¯ä¸ªå¹³å°æœ€å¤š5æ¡
                rank_display = self._format_rank(item.get('rank', 99))
                title = item['title']
                url = item.get('url', '')
                
                if url:
                    title_with_link = f"[{title}]({url})"
                else:
                    title_with_link = title
                    
                message_parts.append(f"  {rank_display} {title_with_link}")
        
        return '\n'.join(message_parts)
    
    def _format_rank(self, rank: int) -> str:
        """æ ¼å¼åŒ–æ’åæ˜¾ç¤º"""
        if rank <= 5:
            return f"**[{rank}]**"
        else:
            return f"[{rank}]"
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def is_enabled(self) -> bool:
        """æ£€æŸ¥ AI å¢å¼ºæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return bool(self.token)