# coding=utf-8

import json
import os
import requests
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import hashlib


class BigModelService:
    """BigModel AI æ™ºèƒ½å»é‡å’Œå†…å®¹åˆ†ææœåŠ¡"""
    
    def __init__(self):
        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        self.model = "glm-4.6"
        self.api_token = os.getenv("CLAUDE_CODE_OAUTH_TOKEN")
        
    def _hash_title(self, title: str) -> str:
        """ç”Ÿæˆæ ‡é¢˜çš„å“ˆå¸Œå€¼ç”¨äºå¿«é€Ÿå»é‡"""
        return hashlib.md5(title.encode('utf-8')).hexdigest()
    
    def _prepare_bigmodel_request(self, messages: List[Dict]) -> Dict:
        """å‡†å¤‡ BigModel API è¯·æ±‚"""
        return {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 8192,
            "stream": False
        }
    
    def _call_bigmodel_api(self, messages: List[Dict]) -> Optional[Dict]:
        """è°ƒç”¨ BigModel API"""
        if not self.api_token:
            print("âš ï¸  æœªè®¾ç½® CLAUDE_CODE_OAUTH_TOKENï¼Œè·³è¿‡ AI åˆ†æ")
            return None
            
        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json"
        }
        
        request_data = self._prepare_bigmodel_request(messages)
        
        try:
            response = requests.post(
                self.api_url, 
                headers=headers, 
                json=request_data,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ BigModel API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ BigModel API è°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_bigmodel_response(self, response: Dict) -> Dict:
        """è§£æ BigModel API å“åº”"""
        try:
            if "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0]["message"]["content"]
                # å°è¯•è§£æ JSON å†…å®¹
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯ JSON æ ¼å¼ï¼Œè¿”å›æ–‡æœ¬å†…å®¹
                    return {
                        "analysis": content,
                        "duplicate_indices": [],
                        "categories": {},
                        "hot_topics": [],
                        "summary": "AI åˆ†æå®Œæˆï¼Œä½†æ ¼å¼è§£æå¤±è´¥"
                    }
        except Exception as e:
            print(f"âŒ BigModel å“åº”è§£æå¤±è´¥: {str(e)}")
            return {
                "analysis": "",
                "duplicate_indices": [],
                "categories": {},
                "hot_topics": [],
                "summary": "AI å“åº”è§£æå¤±è´¥"
            }
    
    def smart_deduplicate_and_analyze(
        self, 
        titles_data: List[Dict]
    ) -> Tuple[List[Dict], Dict]:
        """
        æ™ºèƒ½å»é‡å’Œå†…å®¹åˆ†æ
        
        Args:
            titles_data: åŒ…å«æ ‡é¢˜ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
            æ¯ä¸ªå­—å…¸åŒ…å«: title, source_name, url, mobile_url, ranks, is_new
            
        Returns:
            Tuple[å»é‡åçš„æ ‡é¢˜åˆ—è¡¨, åˆ†æç»“æœå­—å…¸]
        """
        if not self.api_token:
            # é™çº§åˆ°åŸºç¡€å“ˆå¸Œå»é‡
            return self._basic_deduplicate(titles_data), {}
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€å“ˆå¸Œå»é‡
        hash_deduplicated = self._basic_deduplicate(titles_data)
        
        # ç¬¬äºŒæ­¥ï¼šAI è¯­ä¹‰å»é‡å’Œåˆ†æ
        return self._ai_smart_analysis(hash_deduplicated)
    
    def _basic_deduplicate(self, titles_data: List[Dict]) -> List[Dict]:
        """åŸºç¡€å“ˆå¸Œå»é‡"""
        seen_hashes = set()
        deduplicated = []
        
        for item in titles_data:
            title = item.get("title", "")
            title_hash = self._hash_title(title)
            
            if title_hash not in seen_hashes:
                seen_hashes.add(title_hash)
                deduplicated.append(item)
        
        return deduplicated
    
    def _ai_smart_analysis(
        self, 
        titles_data: List[Dict]
    ) -> Tuple[List[Dict], Dict]:
        """AI æ™ºèƒ½åˆ†æ"""
        if not titles_data:
            return [], {}
        
        # å‡†å¤‡ AI åˆ†æè¯·æ±‚
        titles_text = "\n".join([
            f"{i+1}. {item.get('title', '')} (æ¥æº: {item.get('source_name', '')})"
            for i, item in enumerate(titles_data)
        ])
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»å†…å®¹åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»æ ‡é¢˜è¿›è¡Œæ™ºèƒ½åˆ†æï¼š

1. è¯†åˆ«é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–°é—»æ ‡é¢˜ï¼Œè¿”å›éœ€è¦å»é‡çš„æ ‡é¢˜ç´¢å¼•
2. å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»ï¼ˆå¦‚ï¼šç§‘æŠ€ã€è´¢ç»ã€ç¤¾ä¼šã€å›½é™…ã€ä½“è‚²ã€å¨±ä¹ç­‰ï¼‰
3. æå–çƒ­é—¨è¯é¢˜å’Œå…³é”®äº‹ä»¶
4. ç”Ÿæˆç®€è¦çš„å†…å®¹æ€»ç»“

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼š
{
    "duplicate_indices": [1, 3, 5],
    "categories": {
        "ç§‘æŠ€": [2, 4, 7],
        "è´¢ç»": [6, 8],
        "ç¤¾ä¼š": [1, 9]
    },
    "hot_topics": ["AIå‘å±•", "ç»æµæ”¿ç­–", "ç§‘æŠ€åˆ›æ–°"],
    "summary": "ä»Šæ—¥æ–°é—»ä¸»è¦é›†ä¸­åœ¨ç§‘æŠ€åˆ›æ–°å’Œç»æµå‘å±•æ–¹é¢..."
}

æ³¨æ„ï¼š
- duplicate_indices: éœ€è¦ç§»é™¤çš„é‡å¤æ ‡é¢˜ç´¢å¼•ï¼ˆä»1å¼€å§‹è®¡æ•°ï¼‰
- categories: åˆ†ç±»åç§°å’Œå¯¹åº”çš„æ ‡é¢˜ç´¢å¼•åˆ—è¡¨
- hot_topics: 3-5ä¸ªçƒ­é—¨è¯é¢˜
- summary: 50å­—ä»¥å†…çš„å†…å®¹æ€»ç»“"""

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼š

{titles_text}

è¯·è¿”å› JSON æ ¼å¼çš„åˆ†æç»“æœã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # è°ƒç”¨ AI API
        api_response = self._call_bigmodel_api(messages)
        
        if not api_response:
            # API å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
            return titles_data, {}
        
        # è§£æ AI å“åº”
        analysis_result = self._parse_bigmodel_response(api_response)
        
        # æ ¹æ® AI åˆ†æç»“æœå»é‡
        duplicate_indices = set(analysis_result.get("duplicate_indices", []))
        
        # ä¿ç•™æœªè¢«æ ‡è®°ä¸ºé‡å¤çš„æ ‡é¢˜
        deduplicated_titles = [
            title for i, title in enumerate(titles_data) 
            if (i + 1) not in duplicate_indices  # AI ç´¢å¼•ä»1å¼€å§‹
        ]
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯åˆ°æ ‡é¢˜æ•°æ®ä¸­
        categories = analysis_result.get("categories", {})
        for category, indices in categories.items():
            for idx in indices:
                if 1 <= idx <= len(deduplicated_titles):
                    # æ‰¾åˆ°å¯¹åº”çš„æ ‡é¢˜ï¼ˆéœ€è¦è€ƒè™‘å»é‡åçš„ç´¢å¼•å˜åŒ–ï¼‰
                    original_idx = idx - 1
                    if original_idx < len(titles_data):
                        # åœ¨å»é‡åçš„åˆ—è¡¨ä¸­æ‰¾åˆ°è¿™ä¸ªæ ‡é¢˜
                        for title_item in deduplicated_titles:
                            if title_item.get("title") == titles_data[original_idx].get("title"):
                                title_item["ai_category"] = category
                                break
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        original_count = len(titles_data)
        deduplicated_count = len(deduplicated_titles)
        
        analysis_result.update({
            "original_count": original_count,
            "deduplicated_count": deduplicated_count,
            "removed_count": original_count - deduplicated_count,
            "deduplication_rate": f"{((original_count - deduplicated_count) / original_count * 100):.1f}%" if original_count > 0 else "0%"
        })
        
        return deduplicated_titles, analysis_result
    
    def format_ai_enhanced_message(self, titles_data: List[Dict], analysis_result: Dict) -> str:
        """æ ¼å¼åŒ– AI å¢å¼ºçš„æ¶ˆæ¯"""
        if not analysis_result:
            return ""
        
        message_parts = []
        
        # å»é‡ç»Ÿè®¡
        original_count = analysis_result.get("original_count", 0)
        deduplicated_count = analysis_result.get("deduplicated_count", 0) 
        removed_count = analysis_result.get("removed_count", 0)
        deduplication_rate = analysis_result.get("deduplication_rate", "0%")
        
        if removed_count > 0:
            message_parts.append(
                f"ğŸ¤– **AIæ™ºèƒ½å»é‡**: {original_count}æ¡ â†’ {deduplicated_count}æ¡ "
                f"(å»é™¤{removed_count}æ¡é‡å¤ï¼Œå»é‡ç‡{deduplication_rate})"
            )
        
        # å†…å®¹æ€»ç»“
        summary = analysis_result.get("summary", "")
        if summary:
            message_parts.append(f"ğŸ“ **å†…å®¹æ€»ç»“**: {summary}")
        
        # çƒ­é—¨è¯é¢˜
        hot_topics = analysis_result.get("hot_topics", [])
        if hot_topics:
            topics_text = "ã€".join(hot_topics[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            message_parts.append(f"ğŸ”¥ **çƒ­é—¨è¯é¢˜**: {topics_text}")
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = analysis_result.get("categories", {})
        if categories:
            category_stats = []
            for category, indices in categories.items():
                count = len(indices)
                if count > 0:
                    category_stats.append(f"{category}{count}æ¡")
            
            if category_stats:
                stats_text = "ã€".join(category_stats)
                message_parts.append(f"ğŸ·ï¸ **æ™ºèƒ½åˆ†ç±»**: {stats_text}")
        
        # å¹³å°ç»Ÿè®¡
        platform_stats = {}
        for item in titles_data:
            source_name = item.get("source_name", "")
            platform_stats[source_name] = platform_stats.get(source_name, 0) + 1
        
        if platform_stats:
            platform_items = [f"{platform}({count})" for platform, count in sorted(platform_stats.items(), key=lambda x: x[1], reverse=True)[:8]]
            platforms_text = "ã€".join(platform_items)
            message_parts.append(f"ğŸ“Š **å¹³å°åˆ†å¸ƒ**: {platforms_text}")
        
        return "\n\n" + "\n".join(message_parts) if message_parts else ""