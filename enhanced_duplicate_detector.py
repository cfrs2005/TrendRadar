#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºçš„é‡å¤å†…å®¹æ£€æµ‹å’Œæ—¥å¿—ç³»ç»Ÿ
Enhanced duplicate content detection and logging system
"""

import hashlib
import json
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict


@dataclass
class DuplicateRecord:
    """é‡å¤è®°å½•æ•°æ®ç±»"""
    original_title: str
    original_platform: str
    duplicate_title: str
    duplicate_platform: str
    similarity_score: float
    hash_match: bool
    detection_time: str


@dataclass
class DuplicateStats:
    """é‡å¤ç»Ÿè®¡æ•°æ®ç±»"""
    total_processed: int
    total_duplicates: int
    unique_content: int
    platform_duplicates: Dict[str, int]
    cross_platform_duplicates: int
    hash_based_duplicates: int
    similarity_based_duplicates: int
    duplicate_records: List[DuplicateRecord]


class EnhancedDuplicateDetector:
    """å¢å¼ºçš„é‡å¤å†…å®¹æ£€æµ‹å™¨"""
    
    def __init__(self, enable_similarity_check: bool = True):
        self.enable_similarity_check = enable_similarity_check
        self.seen_hashes = set()
        self.seen_titles = set()
        self.duplicate_stats = DuplicateStats(
            total_processed=0,
            total_duplicates=0,
            unique_content=0,
            platform_duplicates={},
            cross_platform_duplicates=0,
            hash_based_duplicates=0,
            similarity_based_duplicates=0,
            duplicate_records=[]
        )
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

    def _generate_content_hash(self, title: str, platform: str = "") -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™ã€å»é™¤ç‰¹æ®Šå­—ç¬¦
        normalized_title = self._normalize_title(title)
        content = f"{normalized_title}|{platform}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def _normalize_title(self, title: str) -> str:
        """æ ‡å‡†åŒ–æ ‡é¢˜"""
        # å»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™ã€å»é™¤å¸¸è§æ— æ„ä¹‰å­—ç¬¦
        normalized = " ".join(title.split()).lower()
        # ç§»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        import re
        normalized = re.sub(r'[^\w\s\u4e00-\u9fff]', '', normalized)
        return normalized.strip()

    def _calculate_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦é‡å åº¦ä½œä¸ºç›¸ä¼¼åº¦æŒ‡æ ‡
        # å®é™…ä½¿ç”¨ä¸­å¯ä»¥é›†æˆæ›´å¤æ‚çš„è¯­ä¹‰ç›¸ä¼¼åº¦ç®—æ³•
        norm1 = self._normalize_title(title1)
        norm2 = self._normalize_title(title2)
        
        if not norm1 or not norm2:
            return 0.0
            
        # è®¡ç®—å­—ç¬¦é‡å åº¦
        set1 = set(norm1)
        set2 = set(norm2)
        intersection = set1 & set2
        union = set1 | set2
        
        return len(intersection) / len(union) if union else 0.0

    def _is_duplicate(self, title: str, platform: str, original_data: Dict[str, Any]) -> Tuple[bool, Optional[DuplicateRecord]]:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹"""
        content_hash = self._generate_content_hash(title, platform)
        normalized_title = self._normalize_title(title)
        
        # 1. ç²¾ç¡®å“ˆå¸ŒåŒ¹é…
        if content_hash in self.seen_hashes:
            # æŸ¥æ‰¾åŸå§‹è®°å½•
            for seen_hash, (orig_title, orig_platform) in getattr(self, '_hash_mapping', {}).items():
                if seen_hash == content_hash:
                    record = DuplicateRecord(
                        original_title=orig_title,
                        original_platform=orig_platform,
                        duplicate_title=title,
                        duplicate_platform=platform,
                        similarity_score=1.0,
                        hash_match=True,
                        detection_time=datetime.now().strftime('%H:%M:%S')
                    )
                    return True, record
        
        # 2. æ ‡å‡†åŒ–æ ‡é¢˜åŒ¹é…
        if normalized_title in self.seen_titles:
            # æŸ¥æ‰¾åŸå§‹è®°å½•
            for seen_title, (orig_title, orig_platform) in getattr(self, '_title_mapping', {}).items():
                if seen_title == normalized_title:
                    record = DuplicateRecord(
                        original_title=orig_title,
                        original_platform=orig_platform,
                        duplicate_title=title,
                        duplicate_platform=platform,
                        similarity_score=1.0,
                        hash_match=False,
                        detection_time=datetime.now().strftime('%H:%M:%S')
                    )
                    return True, record
        
        # 3. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_similarity_check:
            similarity_threshold = 0.8  # å¯é…ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼
            for seen_title, (orig_title, orig_platform) in getattr(self, '_title_mapping', {}).items():
                similarity = self._calculate_similarity(title, orig_title)
                if similarity >= similarity_threshold:
                    record = DuplicateRecord(
                        original_title=orig_title,
                        original_platform=orig_platform,
                        duplicate_title=title,
                        duplicate_platform=platform,
                        similarity_score=similarity,
                        hash_match=False,
                        detection_time=datetime.now().strftime('%H:%M:%S')
                    )
                    return True, record
        
        return False, None

    def add_content(self, title: str, platform: str, data: Dict[str, Any]) -> bool:
        """æ·»åŠ å†…å®¹å¹¶æ£€æµ‹é‡å¤"""
        self.duplicate_stats.total_processed += 1
        
        # åˆå§‹åŒ–æ˜ å°„å­—å…¸
        if not hasattr(self, '_hash_mapping'):
            self._hash_mapping = {}
        if not hasattr(self, '_title_mapping'):
            self._title_mapping = {}
        
        # æ£€æŸ¥é‡å¤
        is_duplicate, duplicate_record = self._is_duplicate(title, platform, data)
        
        if is_duplicate and duplicate_record:
            # è®°å½•é‡å¤
            self.duplicate_stats.total_duplicates += 1
            self.duplicate_stats.duplicate_records.append(duplicate_record)
            
            # ç»Ÿè®¡å¹³å°é‡å¤
            if duplicate_record.original_platform == duplicate_record.duplicate_platform:
                self.duplicate_stats.platform_duplicates[duplicate_record.original_platform] = \
                    self.duplicate_stats.platform_duplicates.get(duplicate_record.original_platform, 0) + 1
            else:
                self.duplicate_stats.cross_platform_duplicates += 1
            
            # ç»Ÿè®¡æ£€æµ‹æ–¹å¼
            if duplicate_record.hash_match:
                self.duplicate_stats.hash_based_duplicates += 1
            else:
                self.duplicate_stats.similarity_based_duplicates += 1
            
            # è®°å½•è¯¦ç»†æ—¥å¿—
            self.logger.info(
                f"ğŸ”„ å‘ç°é‡å¤å†…å®¹ | "
                f"åŸå§‹: [{duplicate_record.original_platform}] {duplicate_record.original_title} | "
                f"é‡å¤: [{duplicate_record.duplicate_platform}] {duplicate_record.duplicate_title} | "
                f"ç›¸ä¼¼åº¦: {duplicate_record.similarity_score:.2f} | "
                f"å“ˆå¸ŒåŒ¹é…: {'æ˜¯' if duplicate_record.hash_match else 'å¦'}"
            )
            
            return False  # æ˜¯é‡å¤å†…å®¹ï¼Œä¸æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
        
        # æ·»åŠ åˆ°å·²è®°å½•å†…å®¹
        content_hash = self._generate_content_hash(title, platform)
        normalized_title = self._normalize_title(title)
        
        self.seen_hashes.add(content_hash)
        self.seen_titles.add(normalized_title)
        self._hash_mapping[content_hash] = (title, platform)
        self._title_mapping[normalized_title] = (title, platform)
        
        self.duplicate_stats.unique_content += 1
        
        # è®°å½•æ–°å¢å†…å®¹æ—¥å¿—
        self.logger.debug(
            f"âœ… æ–°å¢å†…å®¹ | [{platform}] {title}"
        )
        
        return True  # éé‡å¤å†…å®¹

    def get_duplicate_summary(self) -> str:
        """è·å–å»é‡æ‘˜è¦"""
        stats = self.duplicate_stats
        
        if stats.total_duplicates == 0:
            return (
                f"ğŸ¯ **å»é‡æ‘˜è¦**\n"
                f"â€¢ å¤„ç†æ€»æ•°: {stats.total_processed} æ¡\n"
                f"â€¢ é‡å¤å†…å®¹: 0 æ¡\n"
                f"â€¢ ä¿ç•™å†…å®¹: {stats.unique_content} æ¡\n"
                f"â€¢ å»é‡ç‡: 0%"
            )
        
        duplicate_rate = (stats.total_duplicates / stats.total_processed * 100) if stats.total_processed > 0 else 0
        
        summary = [
            f"ğŸ¯ **å»é‡æ‘˜è¦**",
            f"â€¢ å¤„ç†æ€»æ•°: {stats.total_processed} æ¡",
            f"â€¢ é‡å¤å†…å®¹: {stats.total_duplicates} æ¡",
            f"â€¢ ä¿ç•™å†…å®¹: {stats.unique_content} æ¡", 
            f"â€¢ å»é‡ç‡: {duplicate_rate:.1f}%",
            "",
            f"ğŸ” **æ£€æµ‹æ–¹å¼åˆ†å¸ƒ**",
            f"â€¢ å“ˆå¸ŒåŒ¹é…: {stats.hash_based_duplicates} æ¡",
            f"â€¢ ç›¸ä¼¼åº¦åŒ¹é…: {stats.similarity_based_duplicates} æ¡",
            "",
            f"ğŸ“± **å¹³å°é‡å¤åˆ†æ**"
        ]
        
        if stats.platform_duplicates:
            for platform, count in stats.platform_duplicates.items():
                summary.append(f"â€¢ {platform}: {count} æ¡")
        
        if stats.cross_platform_duplicates > 0:
            summary.append(f"â€¢ è·¨å¹³å°é‡å¤: {stats.cross_platform_duplicates} æ¡")
        
        return "\n".join(summary)

    def get_duplicate_details(self) -> str:
        """è·å–é‡å¤å†…å®¹è¯¦ç»†ä¿¡æ¯"""
        if not self.duplicate_stats.duplicate_records:
            return "ğŸ‰ æ­å–œï¼æœ¬æ¬¡æœªå‘ç°é‡å¤å†…å®¹ã€‚"
        
        details = [
            "ğŸ” **é‡å¤å†…å®¹è¯¦æƒ…**",
            ""
        ]
        
        # æŒ‰å¹³å°åˆ†ç»„æ˜¾ç¤º
        platform_groups = {}
        for record in self.duplicate_stats.duplicate_records:
            key = f"{record.original_platform} â†’ {record.duplicate_platform}"
            if key not in platform_groups:
                platform_groups[key] = []
            platform_groups[key].append(record)
        
        for platform_pair, records in platform_groups.items():
            details.append(f"**{platform_pair}** ({len(records)} æ¡):")
            for i, record in enumerate(records[:5], 1):  # æœ€å¤šæ˜¾ç¤º5æ¡
                details.append(
                    f"  {i}. {record.duplicate_title}\n"
                    f"     åŸå§‹: {record.original_title} "
                    f"(ç›¸ä¼¼åº¦: {record.similarity_score:.2f}, "
                    f"æ—¶é—´: {record.detection_time})"
                )
            
            if len(records) > 5:
                details.append(f"  ... è¿˜æœ‰ {len(records) - 5} æ¡é‡å¤å†…å®¹")
            details.append("")
        
        return "\n".join(details)

    def get_stats_dict(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯å­—å…¸"""
        return asdict(self.duplicate_stats)

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.duplicate_stats = DuplicateStats(
            total_processed=0,
            total_duplicates=0,
            unique_content=0,
            platform_duplicates={},
            cross_platform_duplicates=0,
            hash_based_duplicates=0,
            similarity_based_duplicates=0,
            duplicate_records=[]
        )
        self.seen_hashes.clear()
        self.seen_titles.clear()
        if hasattr(self, '_hash_mapping'):
            self._hash_mapping.clear()
        if hasattr(self, '_title_mapping'):
            self._title_mapping.clear()